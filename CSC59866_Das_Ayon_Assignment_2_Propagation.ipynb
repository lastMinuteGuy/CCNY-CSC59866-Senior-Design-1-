{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xmY9J0p4Z-lw"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Proof of BP3\n",
        "\n",
        "Equation BP3 expresses how the partial derivative of the cost function with respect to any weight in the network is calculated:\n",
        "\n",
        "$$\\frac{\\partial C}{\\partial w^l_{jk}} = a^{l-1}_k \\delta^l_j$$\n",
        "\n",
        "#### Derivation:\n",
        "\n",
        "1. The change in the cost $C$ due to a change in weight $w^l_{jk}$ can be expressed through the chain rule as:\n",
        "$$\\frac{\\partial C}{\\partial w^l_{jk}} = \\frac{\\partial C}{\\partial a^l_j} \\frac{\\partial a^l_j}{\\partial z^l_j} \\frac{\\partial z^l_j}{\\partial w^l_{jk}}$$\n",
        "\n",
        "2. Breaking down each component:\n",
        "   - $\\frac{\\partial C}{\\partial a^l_j}$ represents the sensitivity of the cost to the activation of the $j^{th}$ neuron in the $l^{th}$ layer.\n",
        "   - $\\frac{\\partial a^l_j}{\\partial z^l_j}$ is the derivative of the activation function, indicating how the activation changes with respect to the neuron's weighted input.\n",
        "   - $\\frac{\\partial z^l_j}{\\partial w^l_{jk}}$ directly equals $a^{l-1}_k$ because $z^l_j$ is linear with respect to $w^l_{jk}$.\n",
        "\n",
        "3. Combining these expressions gives the BP3 formula:\n",
        "$$\\frac{\\partial C}{\\partial w^l_{jk}} = a^{l-1}_k \\delta^l_j$$\n",
        "\n",
        "This shows that the gradient of the cost with respect to a weight is directly proportional to the activation of the connecting neuron in the previous layer and the error term of the neuron in the current layer.\n",
        "\n",
        "---\n",
        "\n",
        "### Proof of BP4\n",
        "\n",
        "Equation BP4 details the derivative of the cost function with respect to the biases:\n",
        "\n",
        "$$\\frac{\\partial C}{\\partial b^l_j} = \\delta^l_j$$\n",
        "\n",
        "#### Derivation:\n",
        "\n",
        "1. Similarly, applying the chain rule to find how changes in biases affect the cost, we have:\n",
        "$$\\frac{\\partial C}{\\partial b^l_j} = \\frac{\\partial C}{\\partial a^l_j} \\frac{\\partial a^l_j}{\\partial z^l_j} \\frac{\\partial z^l_j}{\\partial b^l_j}$$\n",
        "\n",
        "2. Here:\n",
        "   - $\\frac{\\partial C}{\\partial a^l_j}$ and $\\frac{\\partial a^l_j}{\\partial z^l_j}$ are the same as in BP3.\n",
        "   - $\\frac{\\partial z^l_j}{\\partial b^l_j}$ equals 1 since $z^l_j$ has a linear relationship with $b^l_j$.\n",
        "\n",
        "3. Therefore, we derive BP4 as follows:\n",
        "$$\\frac{\\partial C}{\\partial b^l_j} = \\delta^l_j$$\n",
        "\n",
        "This equation indicates that the gradient of the cost with respect to a bias is equal to the error term of the neuron in the corresponding layer.\n"
      ],
      "metadata": {
        "id": "6lgFqbdbaru5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class SimpleNeuralNetwork:\n",
        "    def __init__(self, sizes):\n",
        "        \"\"\"Initialize the neural network with random weights and biases.\n",
        "\n",
        "        Args:\n",
        "            sizes (list): The sizes of the layers. For example, [2, 3, 1] represents\n",
        "                          a network with 2 inputs, a hidden layer with 3 neurons,\n",
        "                          and an output layer with 1 neuron.\n",
        "        \"\"\"\n",
        "        self.num_layers = len(sizes)\n",
        "        self.sizes = sizes\n",
        "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
        "        self.weights = [np.random.randn(y, x) for x, y in zip(sizes[:-1], sizes[1:])]\n",
        "\n",
        "    def feedforward(self, a):\n",
        "        \"\"\"Return the output of the network given input a.\"\"\"\n",
        "        for b, w in zip(self.biases, self.weights):\n",
        "            a = self.sigmoid(np.dot(w, a) + b)\n",
        "        return a\n",
        "\n",
        "    def sigmoid(self, z):\n",
        "        \"\"\"The sigmoid function.\"\"\"\n",
        "        return 1.0 / (1.0 + np.exp(-z))\n",
        "\n",
        "    def sigmoid_prime(self, z):\n",
        "        \"\"\"Derivative of the sigmoid function.\"\"\"\n",
        "        return self.sigmoid(z) * (1 - self.sigmoid(z))\n",
        "\n",
        "    def backprop(self, x, y):\n",
        "        \"\"\"Return a tuple `(nabla_b, nabla_w)` representing the gradient for the cost function.\n",
        "        `x` is the input data, `y` is the desired output.\n",
        "        \"\"\"\n",
        "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
        "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
        "        # Forward pass\n",
        "        activation = x\n",
        "        activations = [x]  # list to store all the activations, layer by layer\n",
        "        zs = []  # list to store all the z vectors, layer by layer\n",
        "        for b, w in zip(self.biases, self.weights):\n",
        "            z = np.dot(w, activation) + b\n",
        "            zs.append(z)\n",
        "            activation = self.sigmoid(z)\n",
        "            activations.append(activation)\n",
        "\n",
        "        # Backward pass\n",
        "        delta = self.cost_derivative(activations[-1], y) * \\\n",
        "                self.sigmoid_prime(zs[-1])\n",
        "        nabla_b[-1] = delta\n",
        "        nabla_w[-1] = np.dot(delta, activations[-2].T)\n",
        "\n",
        "        for l in range(2, self.num_layers):\n",
        "            z = zs[-l]\n",
        "            sp = self.sigmoid_prime(z)\n",
        "            delta = np.dot(self.weights[-l+1].T, delta) * sp\n",
        "            nabla_b[-l] = delta\n",
        "            nabla_w[-l] = np.dot(delta, activations[-l-1].T)\n",
        "\n",
        "        return (nabla_b, nabla_w)\n",
        "\n",
        "    def cost_derivative(self, output_activations, y):\n",
        "        \"\"\"Return the vector of partial derivatives \\partial C_x / \\partial a for the output activations.\"\"\"\n",
        "        return (output_activations - y)\n"
      ],
      "metadata": {
        "id": "0zI_zrDAa6JQ"
      },
      "execution_count": 1,
      "outputs": []
    }
  ]
}